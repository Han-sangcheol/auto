"""
ë‰´ìŠ¤ í¬ë¡¤ë§ ëª¨ë“ˆ

[íŒŒì¼ ì—­í• ]
ì£¼ìš” ê¸ˆìœµ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

[ì£¼ìš” ê¸°ëŠ¥]
- ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
- ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
- í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
- ì¢…ëª©ì½”ë“œë³„ ë‰´ìŠ¤ í•„í„°ë§
- ì£¼ê¸°ì  ìë™ ê°±ì‹  (ë³„ë„ ìŠ¤ë ˆë“œ)

[ë°ì´í„° í˜•ì‹]
{
    'title': 'ë‰´ìŠ¤ ì œëª©',
    'content': 'ë‰´ìŠ¤ ë³¸ë¬¸',
    'date': 'ë°œí–‰ì¼ì‹œ',
    'source': 'ì¶œì²˜',
    'url': 'URL',
    'related_stocks': ['005930', '000660', ...]
}

[ì‚¬ìš© ë°©ë²•]
crawler = NewsCrawler()
news_list = crawler.get_latest_news('005930')  # ì‚¼ì„±ì „ì ë‰´ìŠ¤
crawler.start_auto_update(interval=300)  # 5ë¶„ë§ˆë‹¤ ìë™ ê°±ì‹ 
"""

from typing import List, Dict, Optional
from datetime import datetime, timedelta
import threading
import time
import re
from collections import defaultdict

try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸  requests, beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install requests beautifulsoup4")

from logger import log


class NewsItem:
    """ë‰´ìŠ¤ ì•„ì´í…œ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        title: str,
        content: str,
        date: datetime,
        source: str,
        url: str,
        related_stocks: List[str] = None
    ):
        self.title = title
        self.content = content
        self.date = date
        self.source = source
        self.url = url
        self.related_stocks = related_stocks or []
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'title': self.title,
            'content': self.content,
            'date': self.date.isoformat() if self.date else None,
            'source': self.source,
            'url': self.url,
            'related_stocks': self.related_stocks
        }
    
    def __repr__(self):
        return f"NewsItem(title='{self.title[:30]}...', source='{self.source}')"


class NewsCrawler:
    """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.news_cache: Dict[str, List[NewsItem]] = defaultdict(list)
        self.last_update_time = None
        self.is_running = False
        self.update_thread = None
        
        # ì¢…ëª©ëª…-ì½”ë“œ ë§¤í•‘ (ì£¼ìš” ì¢…ëª©)
        self.stock_name_map = {
            'ì‚¼ì„±ì „ì': '005930',
            'SKí•˜ì´ë‹‰ìŠ¤': '000660',
            'LGì „ì': '066570',
            'í˜„ëŒ€ì°¨': '005380',
            'ê¸°ì•„': '000270',
            'POSCO': '005490',
            'LGí™”í•™': '051910',
            'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤': '207940',
            'ì¹´ì¹´ì˜¤': '035720',
            'NAVER': '035420',
        }
        
        # ì—­ ë§¤í•‘ (ì½”ë“œ -> ì´ë¦„)
        self.stock_code_map = {v: k for k, v in self.stock_name_map.items()}
        
        if not REQUESTS_AVAILABLE:
            log.warning("âš ï¸  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        else:
            log.info("ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def crawl_naver_finance_news(
        self,
        stock_code: str = None,
        max_count: int = 10
    ) -> List[NewsItem]:
        """
        ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ (Noneì´ë©´ ì „ì²´ ë‰´ìŠ¤)
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if not REQUESTS_AVAILABLE:
            return []
        
        news_list = []
        
        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ URL
            if stock_code:
                # ì¢…ëª© ë‰´ìŠ¤
                url = f"https://finance.naver.com/item/news.naver?code={stock_code}"
            else:
                # ì „ì²´ ì¦ì‹œ ë‰´ìŠ¤
                url = "https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ì°¾ê¸°
            news_items = soup.select('.newsList .articleSubject')
            
            for item in news_items[:max_count]:
                try:
                    # ì œëª© ë° ë§í¬
                    link = item.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://finance.naver.com' + link
                    
                    title = item.get_text(strip=True)
                    
                    # ê°„ë‹¨í•œ ë³¸ë¬¸ ì¶”ì¶œ (ì œëª©ë§Œ ì‚¬ìš©)
                    content = title
                    
                    # ë‚ ì§œ (í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„ì‹œ ì„¤ì •)
                    news_date = datetime.now()
                    
                    # ì¢…ëª© ì½”ë“œ ì¶”ì¶œ (ì œëª©/URLì—ì„œ)
                    related_stocks = []
                    if stock_code:
                        related_stocks = [stock_code]
                    else:
                        # ì œëª©ì—ì„œ ì¢…ëª©ëª… ì°¾ê¸°
                        for stock_name, code in self.stock_name_map.items():
                            if stock_name in title:
                                related_stocks.append(code)
                    
                    news_item = NewsItem(
                        title=title,
                        content=content,
                        date=news_date,
                        source='ë„¤ì´ë²„ê¸ˆìœµ',
                        url=link,
                        related_stocks=related_stocks
                    )
                    
                    news_list.append(news_item)
                    
                except Exception as e:
                    log.debug(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            if news_list:
                log.info(f"âœ… ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            log.error(f"âŒ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return news_list
    
    def crawl_daum_finance_news(
        self,
        stock_code: str = None,
        max_count: int = 10
    ) -> List[NewsItem]:
        """
        ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if not REQUESTS_AVAILABLE:
            return []
        
        news_list = []
        
        try:
            # ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ URL
            if stock_code:
                # A ì ‘ë‘ì‚¬ ì¶”ê°€ (ë‹¤ìŒ ì¢…ëª© ì½”ë“œ í˜•ì‹)
                url = f"https://finance.daum.net/quotes/A{stock_code}#news"
            else:
                url = "https://finance.daum.net/news"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ì°¾ê¸°
            news_items = soup.select('.news_list .link_news')
            
            for item in news_items[:max_count]:
                try:
                    link = item.get('href', '')
                    title = item.get_text(strip=True)
                    
                    # ì¢…ëª© ì½”ë“œ ì¶”ì¶œ
                    related_stocks = []
                    if stock_code:
                        related_stocks = [stock_code]
                    else:
                        for stock_name, code in self.stock_name_map.items():
                            if stock_name in title:
                                related_stocks.append(code)
                    
                    news_item = NewsItem(
                        title=title,
                        content=title,
                        date=datetime.now(),
                        source='ë‹¤ìŒê¸ˆìœµ',
                        url=link,
                        related_stocks=related_stocks
                    )
                    
                    news_list.append(news_item)
                    
                except Exception as e:
                    log.debug(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            if news_list:
                log.info(f"âœ… ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            log.error(f"âŒ ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        
        return news_list
    
    def get_latest_news(
        self,
        stock_code: str = None,
        max_count: int = 20
    ) -> List[NewsItem]:
        """
        ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ì†ŒìŠ¤ í†µí•©)
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ (Noneì´ë©´ ì „ì²´)
            max_count: ìµœëŒ€ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        all_news = []
        
        # ë„¤ì´ë²„ ê¸ˆìœµ
        naver_news = self.crawl_naver_finance_news(stock_code, max_count // 2)
        all_news.extend(naver_news)
        
        # ë‹¤ìŒ ê¸ˆìœµ
        daum_news = self.crawl_daum_finance_news(stock_code, max_count // 2)
        all_news.extend(daum_news)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        all_news.sort(key=lambda x: x.date, reverse=True)
        
        # ìºì‹œì— ì €ì¥
        cache_key = stock_code or 'all'
        self.news_cache[cache_key] = all_news[:max_count]
        self.last_update_time = datetime.now()
        
        return all_news[:max_count]
    
    def get_cached_news(
        self,
        stock_code: str = None
    ) -> List[NewsItem]:
        """
        ìºì‹œëœ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        cache_key = stock_code or 'all'
        return self.news_cache.get(cache_key, [])
    
    def start_auto_update(self, interval: int = 300):
        """
        ìë™ ê°±ì‹  ì‹œì‘ (ë³„ë„ ìŠ¤ë ˆë“œ)
        
        Args:
            interval: ê°±ì‹  ê°„ê²© (ì´ˆ), ê¸°ë³¸ 5ë¶„
        """
        if self.is_running:
            log.warning("ì´ë¯¸ ìë™ ê°±ì‹ ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_running = True
        
        def update_loop():
            log.info(f"ğŸ”„ ë‰´ìŠ¤ ìë™ ê°±ì‹  ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ)")
            
            while self.is_running:
                try:
                    # ì „ì²´ ë‰´ìŠ¤ ê°±ì‹ 
                    self.get_latest_news(max_count=20)
                    log.info(f"âœ… ë‰´ìŠ¤ ìë™ ê°±ì‹  ì™„ë£Œ: {datetime.now().strftime('%H:%M:%S')}")
                    
                    # ëŒ€ê¸°
                    for _ in range(interval):
                        if not self.is_running:
                            break
                        time.sleep(1)
                    
                except Exception as e:
                    log.error(f"âŒ ë‰´ìŠ¤ ìë™ ê°±ì‹  ì˜¤ë¥˜: {e}")
                    time.sleep(interval)
            
            log.info("ğŸ›‘ ë‰´ìŠ¤ ìë™ ê°±ì‹  ì¤‘ì§€")
        
        self.update_thread = threading.Thread(target=update_loop, daemon=True)
        self.update_thread.start()
    
    def stop_auto_update(self):
        """ìë™ ê°±ì‹  ì¤‘ì§€"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        if self.update_thread:
            self.update_thread.join(timeout=5)
        
        log.info("ë‰´ìŠ¤ ìë™ ê°±ì‹ ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_statistics(self) -> Dict:
        """
        í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        total_news = sum(len(news_list) for news_list in self.news_cache.values())
        
        return {
            'total_news': total_news,
            'cached_stocks': list(self.news_cache.keys()),
            'last_update': self.last_update_time.isoformat() if self.last_update_time else None,
            'is_running': self.is_running
        }
    
    def print_news_summary(self, stock_code: str = None):
        """ë‰´ìŠ¤ ìš”ì•½ ì¶œë ¥"""
        news_list = self.get_cached_news(stock_code)
        
        if not news_list:
            print(f"\n{'=' * 60}")
            print("ğŸ“° ë‰´ìŠ¤ ì—†ìŒ")
            print(f"{'=' * 60}\n")
            return
        
        print(f"\n{'=' * 60}")
        print(f"ğŸ“° ìµœì‹  ë‰´ìŠ¤ ({len(news_list)}ê°œ)")
        if stock_code:
            stock_name = self.stock_code_map.get(stock_code, stock_code)
            print(f"ì¢…ëª©: {stock_name} ({stock_code})")
        print(f"{'=' * 60}")
        
        for i, news in enumerate(news_list[:10], 1):
            print(f"\n{i}. [{news.source}] {news.title[:50]}...")
            if news.related_stocks:
                stocks_str = ', '.join([
                    f"{self.stock_code_map.get(code, code)}({code})" 
                    for code in news.related_stocks[:3]
                ])
                print(f"   ê´€ë ¨ ì¢…ëª©: {stocks_str}")
        
        print(f"\n{'=' * 60}\n")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    if not REQUESTS_AVAILABLE:
        print("âš ï¸  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("   pip install requests beautifulsoup4")
        exit(1)
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = NewsCrawler()
    
    # ì‚¼ì„±ì „ì ë‰´ìŠ¤ ìˆ˜ì§‘
    print("\nì‚¼ì„±ì „ì ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    news_list = crawler.get_latest_news('005930', max_count=10)
    
    # ê²°ê³¼ ì¶œë ¥
    crawler.print_news_summary('005930')
    
    # í†µê³„
    stats = crawler.get_statistics()
    print(f"ì´ ë‰´ìŠ¤: {stats['total_news']}ê°œ")
    print(f"ìºì‹œëœ ì¢…ëª©: {', '.join(stats['cached_stocks'])}")
    
    print("\n=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

