"""
ë‰´ìŠ¤ í¬ë¡¤ë§ ëª¨ë“ˆ

[íŒŒì¼ ì—­í• ]
ì£¼ìš” ê¸ˆìœµ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

[ì£¼ìš” ê¸°ëŠ¥]
- ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
- ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§
- í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
- ì¢…ëª©ì½”ë“œë³„ ë‰´ìŠ¤ í•„í„°ë§
- ì£¼ê¸°ì  ìë™ ê°±ì‹  (ë³„ë„ ìŠ¤ë ˆë“œ)

[ë°ì´í„° í˜•ì‹]
{
    'title': 'ë‰´ìŠ¤ ì œëª©',
    'content': 'ë‰´ìŠ¤ ë³¸ë¬¸',
    'date': 'ë°œí–‰ì¼ì‹œ',
    'source': 'ì¶œì²˜',
    'url': 'URL',
    'related_stocks': ['005930', '000660', ...]
}

[ì‚¬ìš© ë°©ë²•]
crawler = NewsCrawler()
news_list = crawler.get_latest_news('005930')  # ì‚¼ì„±ì „ì ë‰´ìŠ¤
crawler.start_auto_update(interval=300)  # 5ë¶„ë§ˆë‹¤ ìë™ ê°±ì‹ 
"""

from typing import List, Dict, Optional
from datetime import datetime, timedelta
import threading
import time
import re
from collections import defaultdict

try:
    import requests
    from bs4 import BeautifulSoup
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸  requests, beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("   pip install requests beautifulsoup4")

from utils.logger import log


class NewsItem:
    """ë‰´ìŠ¤ ì•„ì´í…œ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        title: str,
        content: str,
        date: datetime,
        source: str,
        url: str,
        related_stocks: List[str] = None
    ):
        self.title = title
        self.content = content
        self.date = date
        self.source = source
        self.url = url
        self.related_stocks = related_stocks or []
    
    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'title': self.title,
            'content': self.content,
            'date': self.date.isoformat() if self.date else None,
            'source': self.source,
            'url': self.url,
            'related_stocks': self.related_stocks
        }
    
    def __repr__(self):
        return f"NewsItem(title='{self.title[:30]}...', source='{self.source}')"


class NewsCrawler:
    """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.news_cache: Dict[str, List[NewsItem]] = defaultdict(list)
        self.last_update_time = None
        self.is_running = False
        self.update_thread = None
        
        # ì¢…ëª©ëª…-ì½”ë“œ ë§¤í•‘ (ì£¼ìš” ì¢…ëª©)
        self.stock_name_map = {
            'ì‚¼ì„±ì „ì': '005930',
            'SKí•˜ì´ë‹‰ìŠ¤': '000660',
            'LGì „ì': '066570',
            'í˜„ëŒ€ì°¨': '005380',
            'ê¸°ì•„': '000270',
            'POSCO': '005490',
            'LGí™”í•™': '051910',
            'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤': '207940',
            'ì¹´ì¹´ì˜¤': '035720',
            'NAVER': '035420',
        }
        
        # ì—­ ë§¤í•‘ (ì½”ë“œ -> ì´ë¦„)
        self.stock_code_map = {v: k for k, v in self.stock_name_map.items()}
        
        # ğŸ†• íŒ¨í„´ í•™ìŠµê¸° (ìë™ ë³´ì •)
        try:
            from news_pattern_learner import NewsPatternLearner
            self.pattern_learner = NewsPatternLearner()
            log.info("âœ… ë‰´ìŠ¤ íŒ¨í„´ í•™ìŠµê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            log.warning(f"âš ï¸  íŒ¨í„´ í•™ìŠµê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.pattern_learner = None
        
        # ğŸ†• ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì½œë°± (GUIì— ìƒíƒœ ì „ë‹¬)
        self.monitoring_callback = None
        
        # ì†ŒìŠ¤ë³„ í†µê³„
        self.source_stats = {
            'naver': {'success': 0, 'total': 0},
            'daum': {'success': 0, 'total': 0}
        }
        
        if not REQUESTS_AVAILABLE:
            log.warning("âš ï¸  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
        else:
            log.info("ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def set_monitoring_callback(self, callback):
        """
        ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì½œë°± ì„¤ì •
        
        Args:
            callback: ì½œë°± í•¨ìˆ˜ (message: str, level: str, stock_code: str, source: str)
        """
        self.monitoring_callback = callback
        log.info("ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§ ì½œë°± ì„¤ì • ì™„ë£Œ")
    
    def _log_to_monitor(self, message: str, level: str = "info", stock_code: str = "", source: str = ""):
        """
        ëª¨ë‹ˆí„°ë§ ë¡œê·¸ ì „ì†¡
        
        Args:
            message: ë¡œê·¸ ë©”ì‹œì§€
            level: ë¡œê·¸ ë ˆë²¨ (info, success, warning, error)
            stock_code: ì¢…ëª© ì½”ë“œ
            source: ë‰´ìŠ¤ ì†ŒìŠ¤
        """
        if self.monitoring_callback:
            try:
                self.monitoring_callback(message, level, stock_code, source)
            except Exception as e:
                log.debug(f"ëª¨ë‹ˆí„°ë§ ì½œë°± ì˜¤ë¥˜: {e}")
    
    def crawl_naver_finance_news(
        self,
        stock_code: str = None,
        max_count: int = 10
    ) -> List[NewsItem]:
        """
        ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ (ìë™ ë³´ì • ì ìš©)
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ (Noneì´ë©´ ì „ì²´ ë‰´ìŠ¤)
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if not REQUESTS_AVAILABLE:
            return []
        
        news_list = []
        source_name = "naver"
        self.source_stats[source_name]['total'] += 1
        
        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ URL
            if stock_code:
                url = f"https://finance.naver.com/item/news.naver?code={stock_code}"
            else:
                url = "https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258"
            
            self._log_to_monitor(f"[ë„¤ì´ë²„] ë‰´ìŠ¤ ì¡°íšŒ ì‹œì‘", "info", stock_code or "", source_name)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ğŸ†• íŒ¨í„´ í•™ìŠµê¸°ë¥¼ ì‚¬ìš©í•œ ìë™ ë³´ì •
            news_items = []
            used_selector = '.newsList .articleSubject'  # ê¸°ë³¸ ì…€ë ‰í„°
            
            if self.pattern_learner:
                # 1. ìµœì  ì…€ë ‰í„° ê°€ì ¸ì˜¤ê¸°
                best_selector = self.pattern_learner.get_best_selector(source_name)
                news_items = soup.select(best_selector)
                used_selector = best_selector
                
                # 2. ê²°ê³¼ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì…€ë ‰í„° ì‹œë„
                if not news_items or len(news_items) == 0:
                    self._log_to_monitor(
                        f"[ë„¤ì´ë²„] ê¸°ë³¸ ì…€ë ‰í„° ì‹¤íŒ¨, ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„ ì¤‘...", 
                        "warning", stock_code or "", source_name
                    )
                    
                    working_selector = self.pattern_learner.find_working_selector(source_name, soup)
                    
                    if working_selector:
                        news_items = soup.select(working_selector)
                        used_selector = working_selector
                        self.pattern_learner.record_success(source_name, working_selector)
                        self._log_to_monitor(
                            f"[ë„¤ì´ë²„] ëŒ€ì²´ ì…€ë ‰í„° ì„±ê³µ: {working_selector} ({len(news_items)}ê°œ)",
                            "success", stock_code or "", source_name
                        )
                    else:
                        self.pattern_learner.record_failure(source_name, best_selector)
                        self._log_to_monitor(
                            f"[ë„¤ì´ë²„] ëª¨ë“  ì…€ë ‰í„° ì‹¤íŒ¨",
                            "error", stock_code or "", source_name
                        )
                        return []
                else:
                    # ì„±ê³µ ê¸°ë¡
                    self.pattern_learner.record_success(source_name, best_selector)
            else:
                # íŒ¨í„´ í•™ìŠµê¸° ì—†ìœ¼ë©´ ê¸°ë³¸ ì…€ë ‰í„° ì‚¬ìš©
                news_items = soup.select(used_selector)
            
            # ë‰´ìŠ¤ íŒŒì‹±
            for item in news_items[:max_count]:
                try:
                    link = item.get('href', '')
                    if not link.startswith('http'):
                        link = 'https://finance.naver.com' + link
                    
                    title = item.get_text(strip=True)
                    
                    if not title:  # ë¹ˆ ì œëª©ì€ ê±´ë„ˆë›°ê¸°
                        continue
                    
                    content = title
                    news_date = datetime.now()
                    
                    related_stocks = []
                    if stock_code:
                        related_stocks = [stock_code]
                    else:
                        for stock_name, code in self.stock_name_map.items():
                            if stock_name in title:
                                related_stocks.append(code)
                    
                    news_item = NewsItem(
                        title=title,
                        content=content,
                        date=news_date,
                        source='ë„¤ì´ë²„ê¸ˆìœµ',
                        url=link,
                        related_stocks=related_stocks
                    )
                    
                    news_list.append(news_item)
                    
                    # ìˆ˜ì§‘ ì„±ê³µ ë¡œê·¸
                    self._log_to_monitor(
                        f"[ë„¤ì´ë²„] {title[:40]}...",
                        "info", stock_code or "", source_name
                    )
                    
                except Exception as e:
                    log.debug(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            if news_list:
                self.source_stats[source_name]['success'] += 1
                log.info(f"âœ… ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì…€ë ‰í„°: {used_selector})")
                self._log_to_monitor(
                    f"[ë„¤ì´ë²„] ìˆ˜ì§‘ ì™„ë£Œ: {len(news_list)}ê°œ",
                    "success", stock_code or "", source_name
                )
            
        except Exception as e:
            log.error(f"âŒ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            self._log_to_monitor(
                f"[ë„¤ì´ë²„] í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}",
                "error", stock_code or "", source_name
            )
        
        return news_list
    
    def crawl_daum_finance_news(
        self,
        stock_code: str = None,
        max_count: int = 10
    ) -> List[NewsItem]:
        """
        ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ (ìë™ ë³´ì • ì ìš©)
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if not REQUESTS_AVAILABLE:
            return []
        
        news_list = []
        source_name = "daum"
        self.source_stats[source_name]['total'] += 1
        
        try:
            # ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ URL (ì‹œì¥ êµ¬ë¶„ì ì²˜ë¦¬)
            if stock_code:
                # ì¢…ëª© ì½”ë“œ ì •ë¦¬ (A, Q ì ‘ë‘ì‚¬ ì œê±°)
                clean_code = stock_code.lstrip('AQ')
                
                # ì‹œì¥ êµ¬ë¶„ì ê²°ì • (KOSPI: A, KOSDAQ: Q)
                # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì½”ë“œê°€ 000000~999999ë©´ KOSPI(A), ê·¸ ì™¸ëŠ” í™•ì¸ í•„ìš”
                # ë” ì •í™•í•œ ë°©ë²•ì€ í‚¤ì›€ APIë¡œ ì‹œì¥ êµ¬ë¶„ ì¡°íšŒ
                if clean_code.startswith(('00', '01', '02', '03', '04', '05')):
                    market_prefix = 'A'  # KOSPI
                else:
                    market_prefix = 'Q'  # KOSDAQ (ì¶”ì •)
                
                url = f"https://finance.daum.net/quotes/{market_prefix}{clean_code}#news"
            else:
                url = "https://finance.daum.net/news"
            
            self._log_to_monitor(f"[ë‹¤ìŒ] ë‰´ìŠ¤ ì¡°íšŒ ì‹œì‘: {url}", "info", stock_code or "", source_name)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ğŸ†• íŒ¨í„´ í•™ìŠµê¸°ë¥¼ ì‚¬ìš©í•œ ìë™ ë³´ì •
            news_items = []
            used_selector = '.news_list .link_news'  # ê¸°ë³¸ ì…€ë ‰í„°
            
            if self.pattern_learner:
                best_selector = self.pattern_learner.get_best_selector(source_name)
                news_items = soup.select(best_selector)
                used_selector = best_selector
                
                if not news_items or len(news_items) == 0:
                    self._log_to_monitor(
                        f"[ë‹¤ìŒ] ê¸°ë³¸ ì…€ë ‰í„° ì‹¤íŒ¨, ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„ ì¤‘...",
                        "warning", stock_code or "", source_name
                    )
                    
                    working_selector = self.pattern_learner.find_working_selector(source_name, soup)
                    
                    if working_selector:
                        news_items = soup.select(working_selector)
                        used_selector = working_selector
                        self.pattern_learner.record_success(source_name, working_selector)
                        self._log_to_monitor(
                            f"[ë‹¤ìŒ] ëŒ€ì²´ ì…€ë ‰í„° ì„±ê³µ: {working_selector} ({len(news_items)}ê°œ)",
                            "success", stock_code or "", source_name
                        )
                    else:
                        self.pattern_learner.record_failure(source_name, best_selector)
                        self._log_to_monitor(
                            f"[ë‹¤ìŒ] ëª¨ë“  ì…€ë ‰í„° ì‹¤íŒ¨",
                            "error", stock_code or "", source_name
                        )
                        return []
                else:
                    self.pattern_learner.record_success(source_name, best_selector)
            else:
                news_items = soup.select(used_selector)
            
            # ë‰´ìŠ¤ íŒŒì‹±
            for item in news_items[:max_count]:
                try:
                    link = item.get('href', '')
                    title = item.get_text(strip=True)
                    
                    if not title:
                        continue
                    
                    related_stocks = []
                    if stock_code:
                        related_stocks = [stock_code]
                    else:
                        for stock_name, code in self.stock_name_map.items():
                            if stock_name in title:
                                related_stocks.append(code)
                    
                    news_item = NewsItem(
                        title=title,
                        content=title,
                        date=datetime.now(),
                        source='ë‹¤ìŒê¸ˆìœµ',
                        url=link,
                        related_stocks=related_stocks
                    )
                    
                    news_list.append(news_item)
                    
                    self._log_to_monitor(
                        f"[ë‹¤ìŒ] {title[:40]}...",
                        "info", stock_code or "", source_name
                    )
                    
                except Exception as e:
                    log.debug(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            if news_list:
                self.source_stats[source_name]['success'] += 1
                log.info(f"âœ… ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (ì…€ë ‰í„°: {used_selector})")
                self._log_to_monitor(
                    f"[ë‹¤ìŒ] ìˆ˜ì§‘ ì™„ë£Œ: {len(news_list)}ê°œ",
                    "success", stock_code or "", source_name
                )
            
        except Exception as e:
            log.error(f"âŒ ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            self._log_to_monitor(
                f"[ë‹¤ìŒ] í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}",
                "error", stock_code or "", source_name
            )
        
        return news_list
    
    def get_latest_news(
        self,
        stock_code: str = None,
        max_count: int = 20
    ) -> List[NewsItem]:
        """
        ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ì†ŒìŠ¤ í†µí•©)
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ (Noneì´ë©´ ì „ì²´)
            max_count: ìµœëŒ€ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        all_news = []
        
        # ë„¤ì´ë²„ ê¸ˆìœµ (ì˜ˆì™¸ ì²˜ë¦¬)
        try:
            naver_news = self.crawl_naver_finance_news(stock_code, max_count // 2)
            all_news.extend(naver_news)
        except Exception as e:
            log.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock_code}): {e}")
        
        # ë‹¤ìŒ ê¸ˆìœµ (ì˜ˆì™¸ ì²˜ë¦¬) - í˜„ì¬ URL ë¬¸ì œë¡œ ë¹„í™œì„±í™”
        # try:
        #     daum_news = self.crawl_daum_finance_news(stock_code, max_count // 2)
        #     all_news.extend(daum_news)
        # except Exception as e:
        #     log.debug(f"ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock_code}): {e}")
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        if all_news:
            all_news.sort(key=lambda x: x.date, reverse=True)
        
        # ìºì‹œì— ì €ì¥
        cache_key = stock_code or 'all'
        self.news_cache[cache_key] = all_news[:max_count]
        self.last_update_time = datetime.now()
        
        return all_news[:max_count]
    
    def get_cached_news(
        self,
        stock_code: str = None
    ) -> List[NewsItem]:
        """
        ìºì‹œëœ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            stock_code: ì¢…ëª© ì½”ë“œ
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        cache_key = stock_code or 'all'
        return self.news_cache.get(cache_key, [])
    
    def start_auto_update(self, interval: int = 300):
        """
        ìë™ ê°±ì‹  ì‹œì‘ (ë³„ë„ ìŠ¤ë ˆë“œ)
        
        Args:
            interval: ê°±ì‹  ê°„ê²© (ì´ˆ), ê¸°ë³¸ 5ë¶„
        """
        if self.is_running:
            log.warning("ì´ë¯¸ ìë™ ê°±ì‹ ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_running = True
        
        def update_loop():
            log.info(f"ğŸ”„ ë‰´ìŠ¤ ìë™ ê°±ì‹  ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ)")
            
            while self.is_running:
                try:
                    # ì „ì²´ ë‰´ìŠ¤ ê°±ì‹ 
                    self.get_latest_news(max_count=20)
                    log.info(f"âœ… ë‰´ìŠ¤ ìë™ ê°±ì‹  ì™„ë£Œ: {datetime.now().strftime('%H:%M:%S')}")
                    
                    # ëŒ€ê¸°
                    for _ in range(interval):
                        if not self.is_running:
                            break
                        time.sleep(1)
                    
                except Exception as e:
                    log.error(f"âŒ ë‰´ìŠ¤ ìë™ ê°±ì‹  ì˜¤ë¥˜: {e}")
                    time.sleep(interval)
            
            log.info("ğŸ›‘ ë‰´ìŠ¤ ìë™ ê°±ì‹  ì¤‘ì§€")
        
        self.update_thread = threading.Thread(target=update_loop, daemon=True)
        self.update_thread.start()
    
    def stop_auto_update(self):
        """ìë™ ê°±ì‹  ì¤‘ì§€"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        if self.update_thread:
            self.update_thread.join(timeout=5)
        
        log.info("ë‰´ìŠ¤ ìë™ ê°±ì‹ ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_statistics(self) -> Dict:
        """
        í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        total_news = sum(len(news_list) for news_list in self.news_cache.values())
        
        return {
            'total_news': total_news,
            'cached_stocks': list(self.news_cache.keys()),
            'last_update': self.last_update_time.isoformat() if self.last_update_time else None,
            'is_running': self.is_running
        }
    
    def print_news_summary(self, stock_code: str = None):
        """ë‰´ìŠ¤ ìš”ì•½ ì¶œë ¥"""
        news_list = self.get_cached_news(stock_code)
        
        if not news_list:
            print(f"\n{'=' * 60}")
            print("ğŸ“° ë‰´ìŠ¤ ì—†ìŒ")
            print(f"{'=' * 60}\n")
            return
        
        print(f"\n{'=' * 60}")
        print(f"ğŸ“° ìµœì‹  ë‰´ìŠ¤ ({len(news_list)}ê°œ)")
        if stock_code:
            stock_name = self.stock_code_map.get(stock_code, stock_code)
            print(f"ì¢…ëª©: {stock_name} ({stock_code})")
        print(f"{'=' * 60}")
        
        for i, news in enumerate(news_list[:10], 1):
            print(f"\n{i}. [{news.source}] {news.title[:50]}...")
            if news.related_stocks:
                stocks_str = ', '.join([
                    f"{self.stock_code_map.get(code, code)}({code})" 
                    for code in news.related_stocks[:3]
                ])
                print(f"   ê´€ë ¨ ì¢…ëª©: {stocks_str}")
        
        print(f"\n{'=' * 60}\n")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    if not REQUESTS_AVAILABLE:
        print("âš ï¸  requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("   pip install requests beautifulsoup4")
        exit(1)
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = NewsCrawler()
    
    # ì‚¼ì„±ì „ì ë‰´ìŠ¤ ìˆ˜ì§‘
    print("\nì‚¼ì„±ì „ì ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    news_list = crawler.get_latest_news('005930', max_count=10)
    
    # ê²°ê³¼ ì¶œë ¥
    crawler.print_news_summary('005930')
    
    # í†µê³„
    stats = crawler.get_statistics()
    print(f"ì´ ë‰´ìŠ¤: {stats['total_news']}ê°œ")
    print(f"ìºì‹œëœ ì¢…ëª©: {', '.join(stats['cached_stocks'])}")
    
    print("\n=" * 60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

